{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "OPENWEATHER_API_KEY = os.getenv(\"OPENWEATHER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d21dd8126cf6d023afaf25a027ec5b2\n"
     ]
    }
   ],
   "source": [
    "print(OPENWEATHER_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "if PINECONE_INDEX_NAME not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX_NAME,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read PDF and create documents and upload document into vector store\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    texts = []\n",
    "    for page in reader.pages:\n",
    "        texts.append(page.extract_text())\n",
    "    return texts\n",
    "\n",
    "# Import PDF into vector store\n",
    "pdf_texts = read_pdf('data.pdf')\n",
    "docs = [Document(page_content=text, metadata={}) for text in pdf_texts]\n",
    "docsearch = PineconeVectorStore.from_documents(docs, embedding=embeddings, pinecone_api_key=PINECONE_API_KEY, index_name=PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# os.environ['PINECONE_API_KEY'] = PINECONE_INDEX_NAME\n",
    "docsearch = PineconeVectorStore(index_name=PINECONE_INDEX_NAME, embedding=embeddings, pinecone_api_key=PINECONE_API_KEY)\n",
    "# pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "# docsearch = pc.Index(PINECONE_INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_llm = ChatOpenAI(temperature=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.tools import tool\n",
    "from typing import Annotated\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Define Pinecone tool\n",
    "@tool\n",
    "def pinecone_search(query: Annotated[str, \"The query to search in Pinecone.\"]):\n",
    "    \"\"\"\n",
    "    Search the Pinecone vector database with the provided query.\n",
    "    \n",
    "    Args:\n",
    "    query (str): The query to search in Pinecone.\n",
    "    \n",
    "    Returns:\n",
    "    str: The top result from the Pinecone search.\n",
    "    \"\"\"\n",
    "    response = docsearch.similarity_search(query)\n",
    "    return response\n",
    "\n",
    "# Define OpenWeather tool\n",
    "@tool\n",
    "def get_weather(city: Annotated[str, \"The city to get weather information for.\"]):\n",
    "    \"\"\"\n",
    "    Get the weather information for the specified city using the OpenWeather API.\n",
    "    \n",
    "    Args:\n",
    "    city (str): The city to get weather information for.\n",
    "    \n",
    "    Returns:\n",
    "    dict: The weather information for the specified city.\n",
    "    \"\"\"\n",
    "    api_key = OPENWEATHER_API_KEY\n",
    "    base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
    "    complete_url = base_url + \"appid=\" + api_key + \"&q=\" + city\n",
    "    response = requests.get(complete_url)\n",
    "    return response.json()\n",
    "\n",
    "@tool\n",
    "def dummy_tool(inputs):\n",
    "  \"\"\"\n",
    "  You are given one question and you have to extract city name/name of place from it\n",
    "  Don't respond anything except the city name and don't reply anything if you can't find city name and if the question is about personal information like personal business etc then response with just word business and nothing else.\n",
    "  \n",
    "  \"\"\"\n",
    "  res = openai_llm.invoke(inputs)\n",
    "  return res.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    "    AIMessage\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_openai import ChatOpenAI\n",
    "import functools\n",
    "\n",
    "# Helper function to create a node for a given agent\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    # Convert the agent output into a format that is suitable to append to the global state\n",
    "    if isinstance(result, ToolMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_tools(tools)\n",
    "\n",
    "# Define the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Business agent using Pinecone\n",
    "business_agent = create_agent(\n",
    "    llm,\n",
    "    [pinecone_search],\n",
    "    system_message=\"You should provide business-related information.\"\n",
    ")\n",
    "business_node = functools.partial(agent_node, agent=business_agent, name=\"BusinessAgent\")\n",
    "\n",
    "# Weather agent using OpenWeather API\n",
    "weather_agent = create_agent(\n",
    "    llm,\n",
    "    [get_weather],\n",
    "    system_message=\"You should provide weather-related information.\"\n",
    ")\n",
    "weather_node = functools.partial(agent_node, agent=weather_agent, name=\"WeatherAgent\")\n",
    "\n",
    "# General agent using GPT-4o\n",
    "general_agent = create_agent(\n",
    "    llm,\n",
    "    [dummy_tool],\n",
    "    system_message=\"You should respond to general queries.\"\n",
    ")\n",
    "general_node = functools.partial(agent_node, agent=general_agent, name=\"GeneralAgent\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    sender: str\n",
    "\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define tools\n",
    "tools = [pinecone_search, get_weather, dummy_tool]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# Define the router logic\n",
    "from typing import Literal\n",
    "\n",
    "def router(state) -> Literal[\"call_tool\", \"__end__\", \"BusinessAgent\", \"WeatherAgent\", \"GeneralAgent\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    content = last_message.content.lower()\n",
    "    if last_message.tool_calls:\n",
    "        return \"call_tool\"\n",
    "    elif \"FINAL ANSWER\" in last_message.content:\n",
    "        return \"__end__\"\n",
    "    elif \"weather\" in content:\n",
    "        return \"WeatherAgent\"\n",
    "    elif \"business\" in content:  # Check if the query is business-related\n",
    "        return \"BusinessAgent\"\n",
    "    else:\n",
    "        return \"GeneralAgent\"\n",
    "\n",
    "\n",
    "# Define the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"BusinessAgent\", business_node)\n",
    "workflow.add_node(\"WeatherAgent\", weather_node)\n",
    "workflow.add_node(\"call_tool\", tool_node)\n",
    "workflow.add_node(\"GeneralAgent\", general_node)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"GeneralAgent\",\n",
    "    router,\n",
    "    {\"WeatherAgent\": \"WeatherAgent\",\"BusinessAgent\": \"BusinessAgent\", \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"BusinessAgent\",\n",
    "    router,\n",
    "    {\"WeatherAgent\": \"WeatherAgent\",\"GeneralAgent\": \"GeneralAgent\" , \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"WeatherAgent\",\n",
    "    router,\n",
    "    {\"BusinessAgent\": \"BusinessAgent\",\"GeneralAgent\": \"GeneralAgent\" , \"call_tool\": \"call_tool\", \"__end__\": END}\n",
    ")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    # Each agent node updates the 'sender' field\n",
    "    # the tool calling node does not, meaning\n",
    "    # this edge will route back to the original agent\n",
    "    # who invoked the tool\n",
    "    lambda x: x[\"sender\"],\n",
    "    {\n",
    "        \"BusinessAgent\": \"BusinessAgent\",\n",
    "        \"WeatherAgent\": \"WeatherAgent\",\n",
    "        \"GeneralAgent\": \"GeneralAgent\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(START, \"GeneralAgent\")\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# events = graph.stream(\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             HumanMessage(\n",
    "#                 content=\"weather london\"\n",
    "#             )\n",
    "#         ],\n",
    "#     },\n",
    "#     {\"recursion_limit\": 150},\n",
    "# )\n",
    "# for s in events:\n",
    "#     print(s)\n",
    "#     print(\"----\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "while 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'GeneralAgent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 16\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      6\u001b[0m events \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m      7\u001b[0m     {\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m150\u001b[39m},\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m----\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1110\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m   1107\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m-> 1110\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1780\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step)\u001b[0m\n\u001b[0;32m   1778\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1779\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1780\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:72\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     70\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2576\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2574\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2575\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2576\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2577\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2578\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langgraph\\utils.py:102\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m    101\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m--> 102\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:82\u001b[0m, in \u001b[0;36mBranch._route\u001b[1;34m(self, input, config, reader, writer)\u001b[0m\n\u001b[0;32m     80\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m     81\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minvoke(value, config)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects working\\LangGraph\\.venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:109\u001b[0m, in \u001b[0;36mBranch._finish\u001b[1;34m(self, writer, input, result)\u001b[0m\n\u001b[0;32m    107\u001b[0m     result \u001b[38;5;241m=\u001b[39m [result]\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mends:\n\u001b[1;32m--> 109\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Send) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mends\u001b[49m\u001b[43m[\u001b[49m\u001b[43mr\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     destinations \u001b[38;5;241m=\u001b[39m result\n",
      "\u001b[1;31mKeyError\u001b[0m: 'GeneralAgent'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"while 1\")\n",
    "    content = input(\"you: \")\n",
    "    if content.lower() == \"exit\" or content.lower() == \"quit\":\n",
    "        break\n",
    "    events = graph.stream(\n",
    "        {\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=content #\"What is the weather in New York today?\"\n",
    "                )\n",
    "            ],\n",
    "        },\n",
    "        {\"recursion_limit\": 150},\n",
    "    )\n",
    "    for s in events:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
